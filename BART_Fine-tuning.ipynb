{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config Completer.use_jedi = False\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from datasets import Dataset, DatasetDict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import transformers\n",
    "from datasets import load_dataset, load_metric, load_from_disk\n",
    "import numpy as np\n",
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# df = pd.read_excel('new_keyword_data2.xlsx', engine = 'openpyxl')\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# selected_columns = ['abstract(ddt_bounding_box_distance)', 'new_keyword-gpt4']\n",
    "# filter_df = df[selected_columns]\n",
    "# f_df = filter_df.dropna(axis=0)\n",
    "# f_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = f_df[:4000]\n",
    "test_df = f_df[4000:4400]\n",
    "validation_df = f_df[4400:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pandas DataFrame을 Hugging Face Dataset으로 변환\n",
    "train_dataset = Dataset.from_pandas(train_df)\n",
    "test_dataset = Dataset.from_pandas(test_df)\n",
    "validation_dataset = Dataset.from_pandas(validation_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = DatasetDict({\n",
    "    'train': train_dataset,\n",
    "    'test': test_dataset,\n",
    "    'validation': validation_dataset\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['abstract(ddt_bounding_box_distance)', 'new_keyword-gpt4', '__index_level_0__'],\n",
       "        num_rows: 4000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['abstract(ddt_bounding_box_distance)', 'new_keyword-gpt4', '__index_level_0__'],\n",
       "        num_rows: 400\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['abstract(ddt_bounding_box_distance)', 'new_keyword-gpt4', '__index_level_0__'],\n",
       "        num_rows: 125\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = load_metric('rouge')\n",
    "model_checkpoints = 'facebook/bart-large-xsum'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_input = 512\n",
    "max_target = 128\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(model_checkpoints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(data_to_process):\n",
    "    # 'abstract(ddt_bounding_box_distance)' 텍스트를 입력으로 받음\n",
    "    inputs = [abstract for abstract in data_to_process['abstract(ddt_bounding_box_distance)']]\n",
    "    # 입력 텍스트 토큰화\n",
    "    model_inputs = tokenizer(inputs, max_length=max_input, padding='max_length', truncation=True)\n",
    "\n",
    "    # 'new_keyword'를 타겟으로 설정하고 토큰화\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        targets = tokenizer(data_to_process['new_keyword-gpt4'], max_length=max_target, padding='max_length', truncation=True)\n",
    "    \n",
    "    # 토큰화된 'new_keyword'를 레이블로 설정\n",
    "    model_inputs['labels'] = targets['input_ids']\n",
    "    \n",
    "    # 반환 값에는 입력 토큰, 어텐션 마스크, 레이블이 포함됨\n",
    "    return model_inputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21270f5c1beb46fcbd01d197181849e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ec8b8ad118f4b16b4bfc2213210f14c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b084adc382934841b5cffd4659249f97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "temp_tokenize_data = data.map(preprocess_data, batched=True, remove_columns=['abstract(ddt_bounding_box_distance)', 'new_keyword-gpt4'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d240494cf79f43faaf618d3d09bec04a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98d6180480a0459fbde2f77d5cdb4c5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "118fc36e4c7c408ca3855dc47556c484",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 각 데이터셋에서 '__index_level_0__' 열을 제거\n",
    "tokenize_data = temp_tokenize_data.map(lambda x: x, batched=True, remove_columns=['__index_level_0__'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 4000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 400\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 125\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = transformers.AutoModelForSeq2SeqLM.from_pretrained(model_checkpoints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "collator = transformers.DataCollatorForSeq2Seq(tokenizer, model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_rouge(pred):\n",
    "    predictions, labels = pred\n",
    "    #decode the predictions\n",
    "    decode_predictions = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    #decode labels\n",
    "    decode_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    #compute results\n",
    "    res = metric.compute(predictions=decode_predictions, references=decode_labels, use_stemmer=True)\n",
    "    #get %\n",
    "    res = {key: value.mid.fmeasure * 100 for key, value in res.items()}\n",
    "\n",
    "    pred_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in predictions]\n",
    "    res['gen_len'] = np.mean(pred_lens)\n",
    "\n",
    "    return {k: round(v, 4) for k, v in res.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = transformers.Seq2SeqTrainingArguments(\n",
    "    'conversation-summ',\n",
    "    evaluation_strategy='epoch',\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=4,\n",
    "    gradient_accumulation_steps=2,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=2,\n",
    "    num_train_epochs=10,\n",
    "    predict_with_generate=True,\n",
    "    eval_accumulation_steps=1,\n",
    "    fp16=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainerCallback\n",
    "\n",
    "class SaveBestModelCallback(TrainerCallback):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.best_loss = None\n",
    "\n",
    "    def on_evaluate(self, args, state, control, metrics=None, **kwargs):\n",
    "        # validation loss 추적\n",
    "        val_loss = metrics[\"eval_loss\"]\n",
    "        if self.best_loss is None or val_loss < self.best_loss:\n",
    "            self.best_loss = val_loss\n",
    "            # 새로운 최적 모델 저장\n",
    "            print(f\"New best model found at epoch {state.epoch}. Saving model.\")\n",
    "            model.save_pretrained('./best_bart_deep/')\n",
    "            tokenizer.save_pretrained('./best_bart_deep/')\n",
    "            tokenizer.save_vocabulary('./best_bart_deep/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using amp half precision backend\n"
     ]
    }
   ],
   "source": [
    "trainer = transformers.Seq2SeqTrainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=tokenize_data['train'],\n",
    "    eval_dataset=tokenize_data['validation'],\n",
    "    data_collator=collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_rouge,\n",
    "    callbacks=[SaveBestModelCallback()]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n",
      "***** Running training *****\n",
      "  Num examples = 4000\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 2\n",
      "  Total optimization steps = 1250\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/parallel/_functions.py:65: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1250' max='1250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1250/1250 2:39:14, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rouge1</th>\n",
       "      <th>Rouge2</th>\n",
       "      <th>Rougel</th>\n",
       "      <th>Rougelsum</th>\n",
       "      <th>Gen Len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.479049</td>\n",
       "      <td>42.246700</td>\n",
       "      <td>19.912300</td>\n",
       "      <td>35.402500</td>\n",
       "      <td>35.421500</td>\n",
       "      <td>38.176000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.446398</td>\n",
       "      <td>42.922100</td>\n",
       "      <td>20.298500</td>\n",
       "      <td>36.301100</td>\n",
       "      <td>36.364600</td>\n",
       "      <td>30.824000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.456017</td>\n",
       "      <td>43.239000</td>\n",
       "      <td>19.780000</td>\n",
       "      <td>35.405800</td>\n",
       "      <td>35.564300</td>\n",
       "      <td>34.184000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.536000</td>\n",
       "      <td>0.446125</td>\n",
       "      <td>42.759600</td>\n",
       "      <td>19.445800</td>\n",
       "      <td>35.232600</td>\n",
       "      <td>35.360600</td>\n",
       "      <td>32.136000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.536000</td>\n",
       "      <td>0.447598</td>\n",
       "      <td>42.817300</td>\n",
       "      <td>19.373000</td>\n",
       "      <td>35.319500</td>\n",
       "      <td>35.410900</td>\n",
       "      <td>30.464000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.536000</td>\n",
       "      <td>0.462855</td>\n",
       "      <td>44.301500</td>\n",
       "      <td>19.859800</td>\n",
       "      <td>35.821300</td>\n",
       "      <td>36.010800</td>\n",
       "      <td>32.592000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.536000</td>\n",
       "      <td>0.477572</td>\n",
       "      <td>44.100400</td>\n",
       "      <td>20.588700</td>\n",
       "      <td>35.567500</td>\n",
       "      <td>35.630600</td>\n",
       "      <td>33.192000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.201700</td>\n",
       "      <td>0.485754</td>\n",
       "      <td>43.452800</td>\n",
       "      <td>19.924100</td>\n",
       "      <td>35.177500</td>\n",
       "      <td>35.295200</td>\n",
       "      <td>32.456000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.201700</td>\n",
       "      <td>0.498340</td>\n",
       "      <td>43.169600</td>\n",
       "      <td>19.969400</td>\n",
       "      <td>34.634000</td>\n",
       "      <td>34.725700</td>\n",
       "      <td>32.512000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.201700</td>\n",
       "      <td>0.501812</td>\n",
       "      <td>42.478800</td>\n",
       "      <td>19.157000</td>\n",
       "      <td>34.262400</td>\n",
       "      <td>34.400600</td>\n",
       "      <td>32.584000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 125\n",
      "  Batch size = 4\n",
      "Configuration saved in ./best_bart_deep/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best model found at epoch 1.0. Saving model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./best_bart_deep/pytorch_model.bin\n",
      "tokenizer config file saved in ./best_bart_deep/tokenizer_config.json\n",
      "Special tokens file saved in ./best_bart_deep/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 125\n",
      "  Batch size = 4\n",
      "Configuration saved in ./best_bart_deep/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best model found at epoch 2.0. Saving model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./best_bart_deep/pytorch_model.bin\n",
      "tokenizer config file saved in ./best_bart_deep/tokenizer_config.json\n",
      "Special tokens file saved in ./best_bart_deep/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 125\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to conversation-summ/checkpoint-500\n",
      "Configuration saved in conversation-summ/checkpoint-500/config.json\n",
      "Model weights saved in conversation-summ/checkpoint-500/pytorch_model.bin\n",
      "tokenizer config file saved in conversation-summ/checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in conversation-summ/checkpoint-500/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 125\n",
      "  Batch size = 4\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/parallel/_functions.py:65: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Configuration saved in ./best_bart_deep/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best model found at epoch 4.0. Saving model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./best_bart_deep/pytorch_model.bin\n",
      "tokenizer config file saved in ./best_bart_deep/tokenizer_config.json\n",
      "Special tokens file saved in ./best_bart_deep/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 125\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 125\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 125\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to conversation-summ/checkpoint-1000\n",
      "Configuration saved in conversation-summ/checkpoint-1000/config.json\n",
      "Model weights saved in conversation-summ/checkpoint-1000/pytorch_model.bin\n",
      "tokenizer config file saved in conversation-summ/checkpoint-1000/tokenizer_config.json\n",
      "Special tokens file saved in conversation-summ/checkpoint-1000/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 125\n",
      "  Batch size = 4\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/parallel/_functions.py:65: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 125\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 125\n",
      "  Batch size = 4\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1250, training_loss=0.32634609069824216, metrics={'train_runtime': 9564.6327, 'train_samples_per_second': 4.182, 'train_steps_per_second': 0.131, 'total_flos': 4.334209204224e+16, 'train_loss': 0.32634609069824216, 'epoch': 10.0})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "aabs_list = []\n",
    "kkey_list = []\n",
    "for aabs, kkey in zip(validation_df['abstract(ddt_bounding_box_distance)'], validation_df['new_keyword-gpt4']):\n",
    "    aabs_list.append(aabs)\n",
    "    kkey_list.append(kkey)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_inputs = tokenizer(aabs_list[0], max_length=max_input, padding='max_length', truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [0, 873, 47796, 35, 3112, 8, 28094, 910, 1182, 1848, 14770, 1722, 7948, 36, 282, 12240, 43, 946, 372, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 4363, 21543, 1437, 1437, 1437, 1437, 4198, 13, 3617, 2975, 142, 9, 49, 1337, 27115, 868, 36, 5564, 991, 44431, 1437, 1437, 49041, 43941, 3611, 11, 935, 8, 73, 368, 2472, 433, 6, 53, 49, 2228, 1236, 3358, 267, 6, 2241, 37423, 15, 10, 1810, 1186, 9, 7909, 45451, 1626, 1189, 10, 1233, 910, 1182, 42123, 10, 267, 5457, 361, 12938, 10, 1539, 4, 49, 2187, 819, 16, 22635, 30, 4249, 295, 605, 364, 321, 396, 12, 1090, 8652, 14001, 742, 434, 295, 12240, 16807, 1274, 4, 21887, 6, 14497, 4675, 9, 910, 1182, 1848, 4543, 139, 6, 295, 12240, 195, 155, 417, 14770, 366, 700, 2580, 1437, 1437, 112, 15, 10, 3143, 9, 7909, 45451, 1626, 30, 21495, 10490, 28808, 36, 5618, 43, 36, 1916, 73, 13753, 17, 27, 43, 1437, 1437, 1437, 1437, 515, 9, 5, 295, 605, 6, 8, 31345, 293, 8, 34774, 25510, 3611, 9, 5018, 595, 5018, 13171, 15, 4363, 21543, 2592, 295, 12240, 19, 2200, 9094, 3236, 1021, 801, 321, 4, 245, 36, 705, 1954, 4, 112, 4, 288, 910, 700, 43, 379, 29541, 22987, 1029, 6, 4878, 4, 11, 1285, 6, 5, 44816, 9437, 4596, 31, 5, 24141, 434, 9, 295, 12240, 15, 4543, 139, 6, 14770, 366, 700, 2580, 2386, 7174, 13171, 9, 741, 9697, 38581, 12748, 10, 39872, 4203, 38665, 1345, 260, 4636, 6, 61, 924, 10, 638, 1427, 10875, 5838, 9, 8971, 4, 401, 207, 8, 10, 1427, 12, 40095, 5838, 9, 1814, 4, 176, 207, 23, 112, 4, 1922, 748, 4411, 5, 42914, 18303, 45084, 566, 6, 7, 1248, 6, 5, 431, 1437, 1437, 579, 4363, 21543, 295, 14770, 1722, 7948, 36, 282, 12240, 43, 5473, 11174, 4458, 14, 1395, 28, 1950, 4875, 7, 1248, 30, 15958, 8533, 132, 204, 14194, 731, 231, 36, 119, 705, 73, 29, 43, 290, 158, 842, 13118, 34, 57, 6807, 4, 5, 24141, 434, 5656, 5, 38898, 1258, 16522, 13171, 32, 5, 762, 17294, 1706, 28927, 5, 295, 605, 12720, 4, 5, 29541, 25666, 3435, 2171, 4084, 443, 6, 61, 74, 311, 5693, 741, 9697, 45693, 805, 1345, 260, 19160, 13, 514, 45187, 4, 84, 892, 24130, 14, 215, 10, 16106, 37725, 27015, 4675, 1351, 30, 5, 10, 4779, 9205, 74, 28, 6177, 13, 617, 1810, 1186, 9, 7708, 2975, 4, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Prediction *****\n",
      "  Num examples = 1\n",
      "  Batch size = 4\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1/1 : < :]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "raw_pred, _, _ = trainer.predict([model_inputs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[    2,  9518, 12573,  2893,  6157, 13690,  1722,  1885,     6,\n",
       "        30169, 43262,  6748, 21553,     6,   289,  5906,  4203, 38665,\n",
       "            6, 23124,    12, 46552, 39848,     2,     1,     1,     1,\n",
       "            1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "            1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "            1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "            1,     1,     1,     1,     1,     1,     1,     1]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'</s>Carbon Cloth Nanowire, Atomic Layer Deposition, Heterojunction, Charge-Transfer Efficiency</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(raw_pred[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Atomic Layer Deposition, Hydrothermal Growth, Versatile Substrates, SnO2 Nanowires, Catalysis'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kkey_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
